version: "3.9"

services:
  # SERVICIO 1: HADOOP
  hadoop:
    build:
      context: ./hadoop
    container_name: hadoop
    hostname: hadoop
    # Mapeo de puertos para acceder a las interfaces web (opcional)
    ports:
      - "9870:9870"   # NameNode UI
      - "9864:9864"   # DataNode UI
      - "8032:8032"   # ResourceManager
      - "9000:9000"   # HDFS
      - "22:22"
    networks:
      - bigdata
    volumes:
      # Montar un volumen para persistir los datos HDFS
      - hdfs:/usr/local/hadoop/hadoop_storage
      # Mapea el directorio interno de HDFS al directorio local './hadoop_storage'
      - ./hadoop_storage:/usr/local/hadoop/hadoop_storage

  # SERVICIO 2: BASE DE DATOS MySQL
  mysql:
    image: mysql:8
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: testdb
    ports:
      - "3306:3306"
    networks:
      - bigdata

  # SERVICIO 3: SQOOP
  sqoop:
    build:
      context: ./sqoop
    container_name: sqoop
    depends_on:
      - hadoop
      - mysql
    environment:
      - JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
      - HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
    networks:
      - bigdata
    volumes:
      - ./hadoop/conf:/usr/local/hadoop/etc/hadoop

  # SERVICIO 4: NGNIX: Servidor web de ejemplo para recolectar logs   
  web_server:
    image: nginx:latest
    container_name: web_server
    ports:
      - "8080:80"
    volumes:
      - web_logs:/var/log/nginx  # Ruta donde guardará los logs
      - ./web_server/nginx-custom.conf:/etc/nginx/conf.d/default.conf:ro # Archivo de configuración para Nginx
    networks:
      - bigdata

  # SERVICIO 5: FLUME (Agente flume)
  flume:
    image: bde2020/flume:latest
    container_name: flume
    # Forzar la plataforma para evitar el error de arquitectura
    platform: linux/amd64 
    environment:
      - FLUME_AGENT_NAME=agent1
    volumes:
      - ./flume/agent-config:/opt/flume/agent-config
      - ./flume/log4j.properties:/opt/flume/conf/log4j.properties
      - web_logs:/var/log/nginx:ro # :ro = Read Only: permite que Flume lea los logs pero garantiza que no pueda modificarlos/borrarlos (por error). 
    # Usamos la ruta exacta donde está almacenada la imagen Flume
    command: /usr/local/apache-flume/apache-flume-1.7.0-SNAPSHOT-bin/bin/flume-ng agent -n agent1 -c /opt/flume/conf -f /opt/flume/agent-config/flume-to-hdfs.conf -Dflume.root.logger=INFO,console
    depends_on:
      - hadoop
    networks:
      - bigdata

# VOLÚMENES Y REDES
networks:
  bigdata:
    driver: bridge

volumes:
  hdfs: # Volumen compartido para almacenamiento
  web_logs:  # Volumen compartido para los logs
